{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "976c2ce7-996c-4a9b-92be-fc105e537e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the contents in tables folder\n",
    "dbutils.fs.ls('dbfs:/mnt/mount_s3/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2e33ea-7a90-4d9c-b6ae-337a954d1ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Layer 1 (all attributes are string type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e4e027-eacb-4262-849d-b25acf382ab5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_json, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# event.csv would be read as text file, as it is malformed. The better option is to preprocess this malformed csv, before reading it in the spark. But the assignment says to read it in the # # first layer as it is. \n",
    "df_event = spark.read.text(\"dbfs:/mnt/mount_s3/event.csv\")\n",
    "df_event.columns\n",
    "display(df_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31f99ab-3aea-43a5-bc9c-492c0d02ad97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location for files except event.csv\n",
    "file_location_item = \"dbfs:/mnt/mount_s3/item.csv\"\n",
    "file_location_order = \"dbfs:/mnt/mount_s3/order.csv\"\n",
    "file_location_user = \"dbfs:/mnt/mount_s3/user.csv\"\n",
    "file_type = \"csv\"\n",
    " \n",
    "# CSV options\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    " \n",
    "merged_schema_item = StructType([\n",
    "    StructField(\"adjective\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"modifier\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True)\n",
    "])\n",
    "\n",
    "merged_schema_order = StructType([\n",
    "    StructField(\"InvoiceId\", StringType(), True),\n",
    "    StructField(\"LineItemId\", StringType(), True),\n",
    "    StructField(\"UserId\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"ItemName\", StringType(), True),\n",
    "    StructField(\"ItemCategory\", StringType(), True),\n",
    "    StructField(\"Price\", StringType(), True),\n",
    "    StructField(\"CreatedAt\", StringType(), True),\n",
    "    StructField(\"PaidAt\", StringType(), True)\n",
    "])\n",
    "\n",
    "merged_schema_user = StructType([\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"deleted_at\", StringType(), True),\n",
    "    StructField(\"email_address\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"merged_at\", StringType(), True),\n",
    "    StructField(\"parent_user_id\", StringType(), True)\n",
    "])\n",
    " \n",
    "df_item = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", 'false') \\\n",
    "  .schema(merged_schema_item) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location_item)\n",
    "\n",
    "df_order = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", 'false') \\\n",
    "  .schema(merged_schema_order) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location_order)\n",
    "\n",
    "df_user = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", 'false') \\\n",
    "  .schema(merged_schema_user) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location_user)\n",
    " \n",
    "# Take a look at the data\n",
    "display(df_item)\n",
    "display(df_order)\n",
    "display(df_user)\n",
    "display(df_event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a0af83-1888-47b9-8012-7d8bb5913162",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a766a43-68bd-4475-ba11-9808c7a8575e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists db.event_table;\n",
    "drop table if exists db.order_table;\n",
    "drop table if exists db.item_table;\n",
    "drop table if exists db.user_table;\n",
    "drop table if exists db.event_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8911bf58-1c78-45ee-8271-b34c8e9ee651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event.write.option(\"mergeSchema\", \"true\").mode('overwrite').saveAsTable(\"db.event_table\")\n",
    "df_item.write.mode('overwrite').saveAsTable(\"db.item_table\")\n",
    "df_order.write.mode('overwrite').saveAsTable(\"db.order_table\")\n",
    "df_user.write.mode('overwrite').saveAsTable(\"db.user_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b83204-e456-4bfe-8b6e-e2344792dcde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Layer 2 (data transformed, flattened, parsed with proper datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fef5ba3a-2bfb-4f68-a5c1-54db0b0acb99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from the tables\n",
    "df_event = spark.sql(\"SELECT * FROM db.event_table\")\n",
    "df_item = spark.sql(\"SELECT * FROM db.item_table\")\n",
    "df_order = spark.sql(\"SELECT * FROM db.order_table\")\n",
    "df_user = spark.sql(\"SELECT * FROM db.user_table\")\n",
    "\n",
    "# Show the data from each DataFrame\n",
    "df_event.show()\n",
    "df_item.show()\n",
    "df_order.show()\n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10240d2d-6a4d-4b22-9016-bf5c7c02619d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#All dataset from the previous layer\n",
    "df_item_unprocessed = df_item\n",
    "df_event_unprocessed = df_event\n",
    "df_order_unprocessed = df_order\n",
    "df_user_unprocessed = df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2fac8c1-d5a2-4e03-9ca4-61dc1a76e4cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# process event data \n",
    "df_event_unprocessed = df_event_unprocessed.withColumn(\"id\", monotonically_increasing_id())\n",
    "df_event_unprocessed = df_event_unprocessed.filter(df_event_unprocessed.id > 0).drop(\"id\")\n",
    "\n",
    "# Split whoel text string by \",\"\n",
    "df_event_unprocessed = df_event_unprocessed.withColumn(\"value\", split(df_event_unprocessed[\"value\"], \",\"))\n",
    "\n",
    "df_event_unprocessed = df_event_unprocessed.withColumn(\"event_id\", df_event_unprocessed[\"value\"].getItem(0)) \\\n",
    "                     .withColumn(\"event_time\", df_event_unprocessed[\"value\"].getItem(1)) \\\n",
    "                     .withColumn(\"user_id\", df_event_unprocessed[\"value\"].getItem(2)) \\\n",
    "                     .withColumn(\"event_name\", df_event_unprocessed[\"value\"].getItem(3)) \\\n",
    "                     .withColumn(\"platform\", df_event_unprocessed[\"value\"].getItem(4)) \\\n",
    "                     .withColumn(\"parameter_name\", df_event_unprocessed[\"value\"].getItem(5)) \\\n",
    "                     .withColumn(\"parameter_value\", df_event_unprocessed[\"value\"].getItem(6))\n",
    "\n",
    "# Split each column by \":\"\n",
    "df_event_unprocessed = df_event_unprocessed.withColumn(\"event_name\", split(df_event_unprocessed[\"event_name\"], \":\")) \\\n",
    "                     .withColumn(\"platform\", split(df_event_unprocessed[\"platform\"], \":\")) \\\n",
    "                     .withColumn(\"parameter_name\", split(df_event_unprocessed[\"parameter_name\"], \":\")) \\\n",
    "                     .withColumn(\"parameter_value\", split(df_event_unprocessed[\"parameter_value\"], \":\"))\n",
    "\n",
    "# get second item from the splitted string\n",
    "df_event_unprocessed = df_event_unprocessed.withColumn(\"event_name\", df_event_unprocessed[\"event_name\"].getItem(1)) \\\n",
    "                         .withColumn(\"platform\", df_event_unprocessed[\"platform\"].getItem(1)) \\\n",
    "                         .withColumn(\"parameter_name\", df_event_unprocessed[\"parameter_name\"].getItem(1)) \\\n",
    "                         .withColumn(\"parameter_value\", df_event_unprocessed[\"parameter_value\"].getItem(1))\n",
    "                             \n",
    "\n",
    "# regex remove double quotes \n",
    "df_event_unprocessed = df_event_unprocessed.withColumn('event_name', F.regexp_replace(df_event_unprocessed['event_name'], '\"\"', '')) \\\n",
    "                     .withColumn('platform', F.regexp_replace(df_event_unprocessed['platform'], '\"\"', '')) \\\n",
    "                     .withColumn('parameter_name', F.regexp_replace(df_event_unprocessed['parameter_name'], '\"\"', '')) \\\n",
    "                     .withColumn('parameter_value', F.regexp_replace(df_event_unprocessed['parameter_value'], '\"\"', '')) \\\n",
    "\n",
    "# regex remove \"}\"\n",
    "df_event_unprocessed = df_event_unprocessed.withColumn('parameter_value', F.regexp_replace(df_event_unprocessed['parameter_value'], '}\"', '')).drop('value')\n",
    "\n",
    "# df_event_unprocessed.columns\n",
    "display(df_event_unprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f668a54a-fd60-49b6-92ef-1b5248c8ace6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#All attributes have the common naming convntions\n",
    "df_order_unprocessed = df_order_unprocessed \\\n",
    "    .withColumnRenamed(\"InvoiceId\", \"invoice_id\") \\\n",
    "    .withColumnRenamed(\"LineItemId\", \"line_item_id\") \\\n",
    "    .withColumnRenamed(\"UserId\", \"user_id\") \\\n",
    "    .withColumnRenamed(\"ItemId\", \"item_id\") \\\n",
    "    .withColumnRenamed(\"ItemName\", \"item_name\") \\\n",
    "    .withColumnRenamed(\"ItemCategory\", \"item_category\") \\\n",
    "    .withColumnRenamed(\"Price\", \"price\") \\\n",
    "    .withColumnRenamed(\"CreatedAt\", \"created_at\") \\\n",
    "    .withColumnRenamed(\"PaidAt\", \"paid_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14fe25ca-a237-4a2b-bdd8-106e9f959400",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# This is demonstration of functional usage. I use enumerate and zip methods to loop concurrently over multiple lists. Rest I kept in simple code bloks (without functions). \n",
    "def process_dataframes(df_item_unprocessed: DataFrame, df_event_unprocessed: DataFrame,\n",
    "                       df_order_unprocessed: DataFrame, df_user_unprocessed: DataFrame) -> tuple:\n",
    "    '''\n",
    "    Take dfs and return tuple of processed dfs\n",
    "    '''\n",
    "    schema_event = {\n",
    "        \"event_id\": \"string\",\n",
    "        \"event_time\": \"timestamp\",\n",
    "        \"user_id\": \"integer\",\n",
    "        \"event_name\": \"string\",\n",
    "        \"platform\": \"string\",\n",
    "        \"parameter_name\": \"string\",\n",
    "        \"parameter_value\": \"integer\"\n",
    "    }\n",
    "\n",
    "    schema_item = {\n",
    "        \"adjective\": \"string\",\n",
    "        \"category\": \"string\",\n",
    "        \"created_at\": \"timestamp\",\n",
    "        \"id\": \"integer\",\n",
    "        \"modifier\": \"string\",\n",
    "        \"name\": \"string\",\n",
    "        \"price\": \"integer\"\n",
    "    }\n",
    "\n",
    "    schema_order = {\n",
    "        \"invoice_id\": \"integer\",\n",
    "        \"line_item_id\": \"integer\",\n",
    "        \"user_id\": \"integer\",\n",
    "        \"item_id\": \"integer\",\n",
    "        \"item_name\": \"string\",\n",
    "        \"item_category\": \"string\",\n",
    "        \"price\": \"integer\",\n",
    "        \"created_at\": \"string\",\n",
    "        \"paid_at\": \"string\"\n",
    "    }\n",
    "\n",
    "    schema_user = {\n",
    "        \"created_at\": \"timestamp\",\n",
    "        \"deleted_at\": \"timestamp\",\n",
    "        \"email_address\": \"string\",\n",
    "        \"first_name\": \"string\",\n",
    "        \"id\": \"integer\",\n",
    "        \"last_name\": \"string\",\n",
    "        \"merged_at\": \"string\",\n",
    "        \"parent_user_id\": \"string\"\n",
    "    }\n",
    "\n",
    "    dataframes = [df_event_unprocessed, df_item_unprocessed, df_order_unprocessed, df_user_unprocessed]\n",
    "\n",
    "    schemas = [schema_event, schema_item, schema_order, schema_user]\n",
    "\n",
    "    # loop over schemas and dfs concurrently\n",
    "    for index, (df, schema) in enumerate(zip(dataframes, schemas)):\n",
    "        for col_name, col_type in schema.items():\n",
    "            df = df.withColumn(col_name, col(col_name).cast(col_type))\n",
    "        dataframes[index] = df  # Update the dataframe in the list\n",
    "\n",
    "    # Access the list\n",
    "    df_event_processed, df_item_processed, df_order_processed, df_user_processed = dataframes\n",
    "\n",
    "    # Additional processing for df_order_processed\n",
    "    df_order_processed = df_order_processed.withColumn(\"created_at\", to_timestamp(df_order_processed[\"created_at\"], \"M/d/yyyy H:mm\")) \\\n",
    "                                           .withColumn(\"paid_at\", to_timestamp(df_order_processed[\"paid_at\"], \"M/d/yyyy H:mm\"))\n",
    "\n",
    "    return df_event_processed, df_item_processed, df_order_processed, df_user_processed\n",
    "\n",
    "# Call the function to process the dataframes\n",
    "df_event_processed, df_item_processed, df_order_processed, df_user_processed = process_dataframes(\n",
    "    df_item_unprocessed, df_event_unprocessed, df_order_unprocessed, df_user_unprocessed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06dc0b18-131a-49ce-99d9-6c5acb5e4fb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_event_processed)\n",
    "display(df_item_processed)\n",
    "display(df_order_processed)\n",
    "display(df_user_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f63d9af-722c-4b05-9787-d8ca293cf823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write the data to db, depending in the usage - bronze, silver gold concept of data lake\n",
    "df_event_processed.write.format(\"delta\").mode('overwrite').saveAsTable(\"db.event_processed_table\")\n",
    "df_item_processed.write.format(\"delta\").mode('overwrite').saveAsTable(\"db.item_processed_table\")\n",
    "df_order_processed.write.format(\"delta\").mode('overwrite').saveAsTable(\"db.order_processed_table\")\n",
    "df_user_processed.write.format(\"delta\").mode('overwrite').saveAsTable(\"db.user_processed_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c32840-6b72-4bc7-ae5f-834c7425a9bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Layer 3\n",
    "##### top_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "604d24ab-4766-47f5-85ef-a4ae2420628a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### I will use CTEs here. CTE is used to couple the queries. CTEs are not necessary but if we have large number of queries, we can couple them. This makes the whole flow of information more readble. In the end of layer 3 I will also deconstruct this CTE so as to understand what subquery returns so as to understand more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb20c36-1a90-411d-9ee0-4c091ec68e80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- create a temperory view, this view is the dimension table, just to demonstrate the star schema in a data lake - dimension/fact table\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_test_dim_item AS\n",
    "SELECT id AS item_id,\n",
    "       name AS item_name,\n",
    "       category AS item_category\n",
    "FROM db.item_processed_table;\n",
    "\n",
    "--  this is the CTE, complicated one. but the explanation is given at the end, where I deconstruct it in more simple sql statements. Please follow that.\n",
    "CREATE OR REPLACE TABLE db.fact_top_items_table AS\n",
    "SELECT y.year,\n",
    "       y.item_id,\n",
    "       a.item_name,\n",
    "       y.yearly_total_items_sold,\n",
    "       y.yearly_total_sales,\n",
    "       y.yearly_item_rank,\n",
    "       y.yearly_item_sales_rank,\n",
    "       a.total_items_sold_all_years,\n",
    "       a.total_sales_all_years_per_item,\n",
    "       a.rank_based_on_total_no_of_sales_all_years,\n",
    "       a.rank_based_on_total_sales_all_years\n",
    "FROM (\n",
    "    SELECT YEAR(o.created_at) AS year,\n",
    "           o.item_id,\n",
    "           COUNT(*) AS yearly_total_items_sold,\n",
    "           SUM(o.price) AS yearly_total_sales, \n",
    "           ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY COUNT(*) DESC) AS yearly_item_rank,\n",
    "           ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY SUM(o.price) DESC) AS yearly_item_sales_rank\n",
    "    FROM db.order_processed_table o\n",
    "    JOIN v_test_dim_item i ON o.item_id = i.item_id\n",
    "    GROUP BY YEAR(o.created_at), o.item_id\n",
    ") AS y\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        o.item_name,\n",
    "        o.item_id as item_id,\n",
    "        COUNT(*) AS total_items_sold_all_years,\n",
    "        SUM(o.price) AS total_sales_all_years_per_item,\n",
    "        RANK() OVER (ORDER BY COUNT(*) DESC) AS rank_based_on_total_no_of_sales_all_years,\n",
    "        RANK() OVER (ORDER BY SUM(o.price) DESC) AS rank_based_on_total_sales_all_years\n",
    "    FROM db.order_processed_table o\n",
    "    JOIN v_test_dim_item i ON o.item_id = i.item_id\n",
    "    GROUP BY o.item_name, o.item_id\n",
    ") AS a ON y.item_id = a.item_id\n",
    "ORDER BY y.year DESC, y.item_id;\n",
    "\n",
    "SELECT * FROM db.fact_top_items_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef20807e-c815-45c3-ae5d-498ca044f520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### top_buyers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51024a4e-9876-47d5-9ef1-af224bdb6e27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### This time I will use simple SQL statements instead of CTEs to make things simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e36ebe9c-6abd-4d4b-b69d-abf8aec043af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Create tem view. select from user_processed_table, id, first_name, last_name\n",
    "-- SUM(o.price) AS total_sales_contributed uses aggregate function SUM. SUM(o.price) sums the price column grouped by id. So for all years, for each id it will sum. e.g. year 2013, 2014...\n",
    "--  RANK() OVER (ORDER BY SUM(o.price) DESC) AS sales_rank. Rank() is a window function. Over (ORDER BY SUM(o.price) DESC) gives the rank ordered by highest sum of ittm to lowest sum of item\n",
    "-- MAX(o.created_at) AS last_order_creation_date. It calulates the recent date group by id, so for each item_id\n",
    "-- we are joining user table with order table on user_id and id. This is because both are same (JOINS are done common columns)\n",
    "--  Finally order by max sale to min sales (descending)\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_top_customers AS\n",
    "SELECT\n",
    "    u.id AS user_id,\n",
    "    u.first_name,\n",
    "    u.last_name,\n",
    "    SUM(o.price) AS total_sales_contributed,\n",
    "    RANK() OVER (ORDER BY SUM(o.price) DESC) AS sales_rank,\n",
    "    MAX(o.created_at) AS last_order_creation_date\n",
    "FROM\n",
    "    db.user_processed_table u\n",
    "JOIN\n",
    "    db.order_processed_table o\n",
    "ON\n",
    "    u.id = o.user_id\n",
    "GROUP BY\n",
    "    u.id, u.first_name, u.last_name\n",
    "-- Sort the customers by total sales in descending order\n",
    "ORDER BY\n",
    "    total_sales_contributed DESC\n",
    "-- Select the top 20 customers\n",
    "LIMIT 20;\n",
    "\n",
    "select * from v_top_customers;\n",
    "\n",
    "\n",
    "-- here we are just counting the number of times view_item has occured for each user_id. \n",
    "CREATE OR REPLACE TEMPORARY VIEW v_top_customers_2 AS\n",
    "SELECT e.user_id, COUNT(*) AS view_count\n",
    "FROM db.event_processed_table e\n",
    "WHERE e.event_name = 'view_item'\n",
    "GROUP BY e.user_id\n",
    "ORDER BY view_count DESC\n",
    "LIMIT 20;\n",
    "\n",
    "\n",
    "-- this UNION is not necessary. But the interviewer demands that in the pdf. It is not necessary as there is no commomn user_id where we can make a table join.\n",
    "-- this means the top 20 customers who bought are not necessirily the same top customers who viewed the product.\n",
    "CREATE OR REPLACE TABLE db.top_customers_table AS\n",
    "SELECT user_id, first_name, last_name, total_sales_contributed, sales_rank, last_order_creation_date, NULL AS view_count\n",
    "FROM v_top_customers\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Table 2 (2 columns with NULL placeholders for missing columns)\n",
    "SELECT user_id, NULL AS first_name, NULL AS last_name, NULL AS total_sales_contributed, NULL AS sales_rank, NULL AS last_order_creation_date, view_count\n",
    "FROM v_top_customers_2;\n",
    "\n",
    "-- select * from db.top_customers_table;\n",
    "SELECT* FROM v_top_customers_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb39ff39-92ed-4029-92e5-ff765269c6b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT* FROM v_top_customers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f46a13df-31fe-4667-b1ef-4813f962fac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "787f25b1-f435-45ae-be0f-40fc77a19800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### End of Project\n",
    "##### Now the sample queries, to deconstruct the CTE used in top_item blocks, both output the same result, but in a differnt fashion, one uses CTE, other uses simple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd78902d-ab98-4432-a294-f59e45b739ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_dim_item AS\n",
    "SELECT id AS item_id,\n",
    "       name AS item_name,\n",
    "       category AS item_category\n",
    "FROM db.item_processed_table;\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_all_year AS\n",
    "SELECT\n",
    "    o.item_name,\n",
    "    o.item_id as item_id,\n",
    "    COUNT(*) AS total_items_sold_all_years,\n",
    "    SUM(o.price) AS total_sales_all_years_per_item,\n",
    "    RANK() OVER (ORDER BY COUNT(*) DESC) AS rank_based_on_total_no_of_sales_all_years,\n",
    "    RANK() OVER (ORDER BY SUM(o.price) DESC) AS rank_based_on_total_sales_all_years\n",
    "FROM\n",
    "    db.order_processed_table o\n",
    "JOIN \n",
    "    v_dim_item i ON o.item_id = i.item_id\n",
    "GROUP BY\n",
    "    o.item_name, o.item_id;\n",
    "\n",
    "SELECT \n",
    "       item_name,\n",
    "       item_id,\n",
    "       total_sales_all_years_per_item,\n",
    "       total_items_sold_all_years,\n",
    "       rank_based_on_total_no_of_sales_all_years,\n",
    "       rank_based_on_total_sales_all_years\n",
    "FROM v_all_year\n",
    "ORDER By item_id desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72e79f9-196b-4a06-8cea-ad466e0b3bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "CREATE OR REPLACE TEMPORARY VIEW v_yealy AS\n",
    "SELECT YEAR(o.created_at) AS year,\n",
    "       o.item_id,\n",
    "       COUNT(*) AS yearly_total_items_sold,\n",
    "       SUM(o.price) AS yearly_total_sales, \n",
    "       ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY COUNT(*) desc) AS yearly_item_rank,\n",
    "       ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY SUM(o.price) desc) AS yearly_item_sales_rank\n",
    "FROM db.order_processed_table o\n",
    "JOIN v_dim_item i ON o.item_id = i.item_id\n",
    "GROUP BY YEAR(o.created_at), o.item_id;\n",
    "\n",
    "-- Retrieve the total number of items sold for every year\n",
    "SELECT year,\n",
    "       item_id,\n",
    "       yearly_total_items_sold,\n",
    "       yearly_total_sales,\n",
    "       yearly_item_rank,\n",
    "       yearly_item_sales_rank\n",
    "FROM v_yealy\n",
    "ORDER BY year desc, item_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b5f1d51-4c98-4dfb-9e56-f87e74c35f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_top_mart AS\n",
    "SELECT y.year, y.item_id, a.item_name, y.yearly_total_items_sold, y.yearly_total_sales, y.yearly_item_rank, y.yearly_item_sales_rank,\n",
    "a.total_items_sold_all_years, a.total_sales_all_years_per_item, a.rank_based_on_total_no_of_sales_all_years, a.rank_based_on_total_sales_all_years\n",
    "FROM v_yealy as y\n",
    "LEFT JOIN v_all_year a ON y.item_id = a.item_id;\n",
    "\n",
    "select * from v_top_mart;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "470d6628-68dd-437b-a284-92309828ce0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    ">### Explanation\n",
    "\n",
    "#### Yearly data\n",
    "Some conclusions:\n",
    "1. This is top_mart\n",
    "2. yearly_total_items_sold: This is `count(*)` window function grouped by - `GROUP BY YEAR(o.created_at)`, o.item_id. so each year for each item_id. \n",
    "that is, for year 2013, 2016 etc and for each item_id in this case 3397 it is showing this 1 in 2013, 1 in 2016, 5 in 2018 etc. Try this code:\n",
    "\n",
    "            %sql\n",
    "            select * from v_top_mart\n",
    "            where item_id=3997;\n",
    "\n",
    "3. yearly_total_sales:  this `SUM(o.price)`  window function again - `GROUP BY YEAR(o.created_at)`, o.item_id. so for each year and for each item_id.\n",
    "so, for year 2013, 3, price is 3 which in the subsequent code. For 2014: two times in 2014 so 3+3 = 6 and so on.\n",
    "\n",
    "            %sql\n",
    "            select * from db.order_processed_table o\n",
    "            inner join db.item_processed_table i on o.item_id = i.id\n",
    "            where i.id=3997\n",
    "\n",
    "4. yearly_item_rank:    `ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY COUNT(*) desc) AS yearly_item_rank`. This means it is using row_number() window function.\n",
    "`row_number` takes `over()` clause where it partition the dataset according to that clause, so `OVER (PARTITION BY YEAR(o.created_at))` partitions the whole dataset according to year. In addition,\n",
    "`ORDER BY COUNT(*) desc in over()` clause orders the data in descending order of total items `(count(*))`. \n",
    "So `count(*)` will calculate for each year total number of occurance of each item_id. Then it puts it in the desc order so that max is put on the top, and row_number put first rank for that max number of occurnce.  \n",
    "\n",
    "So for example, in this query:\n",
    "\n",
    "      %sql\n",
    "      SELECT YEAR(o.created_at) AS year,\n",
    "            o.item_id,\n",
    "            COUNT(*) AS yearly_total_items_sold,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY COUNT(*) desc) AS yearly_item_rank\n",
    "            FROM db.order_processed_table o\n",
    "            JOIN v_dim_item i ON o.item_id = i.item_id\n",
    "            GROUP BY YEAR(o.created_at), o.item_id;\n",
    "\n",
    " the number of occurance of Item_id 3192 is added up for 2013 , and its wrutten in teh results: yearly_total_items_sold =6. this is computed by:  `COUNT(*) AS yearly_total_items_sold,GROUP BY YEAR(o.created_at), o.item_id;`  specifically `COUNT(*)` clauses in the above query. Now  `ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY COUNT(*) desc)`. Now, `ROW_NUMBER()` assigns for that item 3192 a rank based on the occurance of 6 (total occurance of item_id=6) which is 1. Desc oreder someting like this: 6, 5, 4, 3. So max gets the rank1. \n",
    "\n",
    "5. yearly_item_sales_rank: works with two commands/queries: `ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY SUM(o.price) desc) AS yearly_item_sales_rank` && `GROUP BY YEAR(o.created_at), o.item_id; SUM(o.price)` calculates for each year or specific year, the total number sales for each item. `ROW_NUMBERR()` assigns teh rank based in the that total sale of each item in that year in desc  order, so max price goes on the top and that price is assiged number 1 \n",
    "\n",
    "            %sql\n",
    "            SELECT YEAR(o.created_at) AS year,\n",
    "                  o.item_id,\n",
    "                  COUNT(*) AS yearly_total_items_sold,\n",
    "                  SUM(o.price) AS yearly_total_sales, \n",
    "                  ROW_NUMBER() OVER (PARTITION BY YEAR(o.created_at) ORDER BY SUM(o.price) desc) AS yearly_item_sales_rank\n",
    "                  FROM db.order_processed_table o\n",
    "                  JOIN v_dim_item i ON o.item_id = i.item_id\n",
    "                  GROUP BY YEAR(o.created_at), o.item_id;\n",
    "\n",
    "                  --output: \n",
    "                  --     year item_id yearly_total_item_sold yearlyu_total_sales yearly_item_sales_rank\n",
    "                  --     2013 2592      2                       2500               1\n",
    "                  --     2013 844       2                       1800               2\n",
    "\n",
    "make sense:)\n",
    "\n",
    "\n",
    "#### All year data\n",
    "\n",
    "1. total_items_sold_all_years: this is total in all years, so each of the item_id shoudl be summed up. Simple. It is not grouped by created_date. It is just grouped by item_id:\n",
    "`COUNT(*) AS total_items_sold_all_years, GROUP BY o.item_name, o.item_id;` You can gorup by only by item_id. And it will work.\n",
    "\n",
    "2.  total_sales_all_years_per_item: again `SUM(o.price) AS total_sales_all_years_per_item, grouped by item_id`.  easy peasy.\n",
    "3. rank_based_on_total_no_of_sales_all_years: `RANK() OVER (ORDER BY COUNT(*) DESC) AS rank_based_on_total_no_of_sales_all_years`, here also we are grouping by item_id. and `COUNT(*)` will count the occurance of each item_id in total, not in each year, but in all the years that item woudl be found, so for example, 2013, 2014, 2015 and until the end. the count will assigned in tyhe desc order so max number of occurance will be on the top, finally `RANK()` will give it the rank. rank, dense_rank and row number are nealry same, check here for explanation:https://stackoverflow.com/questions/7747327/sql-rank-versus-row-number\n",
    "\n",
    "4. `RANK() OVER (ORDER BY SUM(o.price) DESC) AS rank_based_on_total_sales_all_years`, this calculates price of each item in all years (total) and then orders each item regardless of year in desc order and ranks max price of on top then ranks each item by item_id and gives rank. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ef8e18-29a7-4220-9859-6b91c7692ee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "final_ Notebook 2023-10-15 09_13_54 (2) (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
